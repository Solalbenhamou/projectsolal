#!/usr/bin/env python3
"""
Script autonome pour analyser le churn par groupe et générer graphiques et CSVs.
"""
import os
import sys
import argparse
import logging
from google.cloud import bigquery
from google.cloud.bigquery import QueryJobConfig, ScalarQueryParameter
import pandas as pd
import matplotlib.pyplot as plt

# Config du logger
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

def fetch_predictions(client):
    """
    Récupère toutes les prédictions de churn depuis BigQuery.
    """
    logging.info("Récupération des prédictions de churn...")
    sql = """
    SELECT
      shop_id,
      run_date,
      proba_churn,
      group_number
    FROM
      `warehouse-459819.Predictions.prediction_group_enriched`
    """
    return client.query(sql).to_dataframe()


def fetch_shop_ids(client, shop_name: str):
    """
    Récupère les shop_id correspondant au nom de l'entreprise (insensible à la casse).
    """
    logging.info(f"Recherche des shop_id pour '{shop_name}'...")
    sql = """
    SELECT shop_id
    FROM `warehouse-459819.analytics_test.stg_shop`
    WHERE LOWER(shop_name) = LOWER(@shop_name)
    """
    job_config = QueryJobConfig(
        query_parameters=[
            ScalarQueryParameter("shop_name", "STRING", shop_name)
        ]
    )
    return client.query(sql, job_config=job_config).to_dataframe()


def process_shop(df_preds: pd.DataFrame, shop_id: int, shop_name: str, threshold: float, output_dir: str):
    """
    Filtre les données pour un shop donné, trace et enregistre les résultats.
    """
    logging.info(f"Traitement du shop {shop_name} (ID:{shop_id}) avec seuil {threshold*100:.0f}%...")
    df = df_preds.copy()

    # Gestion des dates et fuseau
    df['run_date'] = pd.to_datetime(df['run_date'], errors='coerce')
    if df['run_date'].dt.tz is None:
        df['run_date'] = df['run_date'].dt.tz_localize('UTC', ambiguous='NaT')
    df['run_date'] = df['run_date'].dt.tz_convert('Asia/Jerusalem')
    today = pd.Timestamp.now('Asia/Jerusalem').normalize()

    # Filtrer sur la journée courante
    mask = (
        (df['shop_id'] == shop_id) &
        (df['run_date'] >= today) &
        (df['run_date'] < today + pd.Timedelta(days=1))
    )
    df_today = df.loc[mask]

    # Compter les proba_churn supérieurs au seuil par groupe
    counts_over = (
        df_today
        .groupby('group_number')['proba_churn']
        .apply(lambda s: (s > threshold).sum())
        .loc[lambda s: s > 0]
    )
    if counts_over.empty:
        logging.info(f"Aucun churn > {threshold*100:.0f}% pour {shop_name} (ID:{shop_id})")
        return

    # Tracé et enregistrement
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.bar(counts_over.index.astype(str), counts_over.values)
    ax.set(
        xlabel='Group Number',
        ylabel=f"Clients (proba_churn > {threshold*100:.0f}%)",
        title=f"{shop_name} (ID:{shop_id}) — churn > {threshold*100:.0f}%"
    )
    fig.tight_layout()

    # Construction des chemins de sortie
    png_path = os.path.join(output_dir, f"{shop_name}_{shop_id}_churn.png")
    csv_path = os.path.join(output_dir, f"{shop_name}_{shop_id}_churn_counts.csv")
    try:
        fig.savefig(png_path)
        counts_over.to_csv(csv_path, header=['count'])
        logging.info(f"Résultats enregistrés : {png_path}, {csv_path}")
    except Exception as e:
        logging.error(f"Erreur enregistrement fichiers : {e}")
    finally:
        plt.close(fig)


def main():
    parser = argparse.ArgumentParser(
        description="Analyse quotidienne du churn par groupe pour un shop donné"
    )
    parser.add_argument(
        '--shop_name', required=True,
        help="Nom de l'entreprise (insensible à la casse)"
    )
    parser.add_argument(
        '--threshold_pct', type=float, required=True,
        help="Seuil de churn en pourcentage (ex: 80 pour 80%)"
    )
    parser.add_argument(
        '--output_dir', default='outputs',
        help="Dossier de destination pour les fichiers générés"
    )
    args = parser.parse_args()

    # Vérification des variables d'environnement
    if not os.getenv('GOOGLE_APPLICATION_CREDENTIALS'):
        logging.warning("La variable GOOGLE_APPLICATION_CREDENTIALS n'est pas définie.")
    if not os.getenv('GCP_PROJECT'):
        logging.warning("La variable GCP_PROJECT n'est pas définie. Le client utilisera la configuration par défaut.")

    os.makedirs(args.output_dir, exist_ok=True)

    # Initialisation du client BigQuery
    try:
        client = bigquery.Client()
    except Exception as e:
        logging.error(f"Impossible d'initialiser BigQuery Client : {e}")
        sys.exit(1)

    # Récupération et traitement
    try:
        df_preds = fetch_predictions(client)
        df_shops = fetch_shop_ids(client, args.shop_name)
    except Exception as e:
        logging.error(f"Erreur lors des requêtes BigQuery : {e}")
        sys.exit(1)

    if df_shops.empty:
        logging.error(f"Aucun shop_id trouvé pour '{args.shop_name}'")
        sys.exit(1)

    threshold = args.threshold_pct / 100
    for sid in df_shops['shop_id']:
        process_shop(df_preds, sid, args.shop_name, threshold, args.output_dir)

if __name__ == '__main__':
    main()
